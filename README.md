# Selective Amnesia in Conditional VAEs

This repository implements a **Machine Unlearning** pipeline for Conditional Variational Autoencoders (CVAE) on the MNIST dataset. It simulates and reproduces the core findings of the paper **"Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models"**.

**Note:** This project extends the original methodology with novel visualization techniquesâ€”specifically **Latent Label Interpolation**â€”to provide a more rigorous verification of the unlearning process.

## ðŸ” Project Overview

The goal is to "surgically" remove specific concepts (e.g., specific digits) from a trained Generative Model without retraining it from scratch. We achieve this using a hybrid loss objective:

1.  **Surrogate Optimization:** Forcing the target class to map to a maximum entropy distribution (Uniform Noise).
2.  **Elastic Weight Consolidation (EWC):** Penalizing changes to parameters critical for previous knowledge (using Fisher Information).
3.  **Generative Replay:** Using a frozen copy of the original model to rehearse non-forgotten concepts.

## ðŸ“‚ Repository Structure

```text
â”œâ”€â”€ model.py           # OneHotCVAE Architecture (Expanding MLP)
â”œâ”€â”€ utils.py           # Helper functions for Fisher Calculation
â”œâ”€â”€ plotting.py        # Visualization tools (UMAP, PCA-Grids, Morphing)
â”œâ”€â”€ main.py            # Execution Pipeline (Training -> Fisher -> Unlearning -> Eval)
â””â”€â”€ results/           # Generated Analysis
    â”œâ”€â”€ original_latent.png
    â”œâ”€â”€ amnesia_latent_01.png
    â”œâ”€â”€ amnesia_morph_0.png
    â””â”€â”€ amnesia_morph_9.png
